{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a691bd4-9a32-4d9a-8a6a-6afd3d1c0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as pa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "from pandas.api.types import is_string_dtype\n",
    "# import seaborn as sn\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import sqrt\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import tsfresh\n",
    "from shapely.geometry import mapping\n",
    "from tqdm.contrib.concurrent import process_map \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d54ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from pygeosys.timeserie.smoothers import  whitw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12508d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:/Users/lwh/Documents/PROJECTS/tillage_detection/truterra_carbon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab679a0-92d7-44d1-bc46-17ec7a548a22",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497c703-fbef-41fd-aec8-30912e17a5c3",
   "metadata": {},
   "source": [
    "### Prepare TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smoother(x,beta = 10000):\n",
    "    weights = (~x[1].isna()).astype(int)\n",
    "    return whitw(x[1].fillna(0.0).values, weights.values, alpha=3, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526575cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataframe for several years\n",
    "\n",
    "# with pickle\n",
    "with open(\"data/02_clean/2016-to-2020-SAR_training_corrected_angle_indices_remaped.pkl\", \"rb\") as f:\n",
    "    gdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_convert = ['VH-max','VH-mean','VH-median','VH-min','VH-stdDev','VV-max','VV-mean','VV-median','VV-min','VV-stdDev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for band in cols_to_convert:\n",
    "    gdf[band]=decibel_to_linear(gdf[band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to time serie readable format\n",
    "gdf['date'] = pd.to_datetime(gdf['date'], format='%Y%m%d') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_2017 = gdf[(gdf['date']<'2017-06-30') ]\n",
    "gdf_2018 = gdf[(gdf['date']>'2017-06-30') & (gdf['date']<'2018-06-30') ]\n",
    "gdf_2019 = gdf[(gdf['date']>'2018-06-30') & (gdf['date']<'2019-06-30') ]\n",
    "gdf_2020 = gdf[(gdf['date']>'2019-06-30') ]\n",
    "gdf_2017.set_index('date',inplace=True)\n",
    "gdf_2018.set_index('date',inplace=True)\n",
    "gdf_2019.set_index('date',inplace=True)\n",
    "gdf_2020.set_index('date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ea9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_interpolate = ['DiffVVVH-max','DiffVVVH-mean','DiffVVVH-median','DiffVVVH-min','DiffVVVH-stdDev','VH-max',\n",
    " 'VH-mean','VH-median','VH-min','VH-stdDev','VV-max','VV-mean','VV-median','VV-min','VV-stdDev','angle-max','angle-mean','angle-median',\n",
    " 'angle-min','angle-stdDev'] # Change your columns here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_gdf(gdf,cols_to_interpolate):\n",
    "    fields = gdf['id'].unique()\n",
    "\n",
    "    #1st field\n",
    "    gdf_sub = gdf.loc[gdf['id']==fields[0]]\n",
    "    resampled = gdf_sub.resample('D').asfreq()#'Y' for Yearly\n",
    "    for col in cols_to_interpolate:\n",
    "        for row in resampled.loc[:,[f'{col}']].items():\n",
    "            resampled[f'{col}'] = apply_smoother(row,beta=1000)\n",
    "    #other fields\n",
    "    for field in tqdm(fields[1:]) : \n",
    "        try: \n",
    "            gdf_sub = gdf.loc[gdf['id']==field]\n",
    "            resampled_tmp = gdf_sub.resample('D').asfreq()#'Y' for Yearly\n",
    "            for col in cols_to_interpolate:\n",
    "                for row in resampled_tmp.loc[:,[f'{col}']].items():\n",
    "                    resampled_tmp[f'{col}'] = apply_smoother(row,beta=1000)\n",
    "\n",
    "            resampled = pd.concat([resampled,resampled_tmp],axis=0)\n",
    "\n",
    "        except:\n",
    "            print(field)\n",
    "    return(resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_2017 = treat_gdf(gdf_2017,cols_to_interpolate)\n",
    "resampled_2018 = treat_gdf(gdf_2018,cols_to_interpolate)\n",
    "resampled_2019 = treat_gdf(gdf_2019,cols_to_interpolate)\n",
    "resampled_2020 = treat_gdf(gdf_2020,cols_to_interpolate)\n",
    "gdf_resampled = pd.concat([resampled_2017,resampled_2018,resampled_2019,resampled_2020],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a38050",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_resampled[['id', 'TillageType', 'Tillagedate', 'CoverCrop', 'field_id', 'State',\n",
    "       'Tillage_newType']] = gdf_resampled[['id', 'TillageType', 'Tillagedate', 'CoverCrop', 'field_id', 'State',\n",
    "       'Tillage_newType']].ffill() # to complete the qualitative features in the dataframe because the upsampled left some Nans\n",
    "gdf_resampled.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523b1c0",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f1a52-01a5-4059-a3ae-749c7373c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe work\n",
    "gdf=gdf_resampled\n",
    "# filter data by date if required\n",
    "gdf['date'] = pd.to_datetime(gdf['date'], format='%Y%m%d') \n",
    "\n",
    "gdf_train=gdf[(gdf['date']<'2019-10-30') ]\n",
    "gdf_test=gdf[gdf['date']>'2019-10-30']\n",
    "\n",
    "# drop unwanted columns \n",
    "timeseries_train = gdf_train.drop(['field_id','State','Tillage_newType'],axis=1)\n",
    "\n",
    "timeseries_test = gdf_test.drop(['field_id','State','Tillage_newType'],axis=1)\n",
    "\n",
    "timeseries = gdf.drop(['field_id','State','Tillage_newType'],axis=1)\n",
    "\n",
    "# store labels (could store several different labels if needed )\n",
    "y_train=gdf_train[['id','Tillage_newType']] # field id and targets columns\n",
    "y_train=y_train.drop_duplicates(subset=['id'],keep='first') # keep only first occurence of field to get the target label\n",
    "y_train=y_train.set_index('id')\n",
    "y_train=y_train['Tillage_newType'] #target column\n",
    "\n",
    "y_test=gdf_test[['id','Tillage_newType']] #field id and targets columns\n",
    "y_test=y_test.drop_duplicates(subset=['id'],keep='first')\n",
    "y_test=y_test.set_index('id')\n",
    "y_test=y_test['Tillage_newType']#target column\n",
    "\n",
    "#prepare time serie\n",
    "timeseries_train=timeseries_train.set_index(['date'])\n",
    "timeseries_test=timeseries_test.set_index(['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff9d9f-3bb9-46a1-9174-420812486fac",
   "metadata": {},
   "source": [
    "### Plot timeserie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea43de-ae0b-45f1-b248-d55bd617254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time series to check if everything is in order\n",
    "timeseries[timeseries['id'] == 90].plot(subplots=True, sharex=True, figsize=(10,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d8fd3-6241-4d6b-8c9d-4502edd96ed0",
   "metadata": {},
   "source": [
    "### Handle NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf1039-6e80-47b5-a55d-8ad0a3fca977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the whole timeserie\n",
    "for col in timeseries.columns :\n",
    "    timeseries[col].fillna((timeseries[col].mean()), inplace=True)\n",
    "    \n",
    "# remove date index\n",
    "timeseries=timeseries.reset_index(0)\n",
    "\n",
    "# in specific timeseries\n",
    "for col in timeseries_train.columns :\n",
    "    timeseries_train[col].fillna((timeseries_train[col].mean()), inplace=True)\n",
    "# remove date index\n",
    "timeseries_train=timeseries_train.reset_index(0)\n",
    "\n",
    "for col in timeseries_test.columns :\n",
    "    timeseries_test[col].fillna((timeseries_test[col].mean()), inplace=True)\n",
    "   \n",
    "# remove date index\n",
    "timeseries_test=timeseries_test.reset_index(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4f958-907a-4ffc-9eb5-d823411176db",
   "metadata": {},
   "source": [
    "### Training classifier with a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109c725-b0f6-45b2-a91a-17ecbdad69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tsfresh.transformers import RelevantFeatureAugmenter\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('augmenter', RelevantFeatureAugmenter(column_id='id', column_sort='date')),\n",
    "            ('classifier', RandomForestClassifier())])\n",
    "X_train = pd.DataFrame(index=y_train.index)\n",
    "\n",
    "pipeline.set_params(augmenter__timeseries_container=timeseries_train) ## /!\\ very important step : make sure the right timeserie is used here\n",
    "pipeline.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ad65a-a526-4ded-82da-43a6e03cb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model pipeline\n",
    "import pickle\n",
    "with open(\"data/07_models/SAR-2019correctedangle_pipeline_resampled_dates_interpol_linear\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f)  # to save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5864b1-99f1-40c7-a0e8-a38b938673d2",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119ec75-eeba-4bdf-8363-7f9d0da510b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "pipeline.set_params(augmenter__timeseries_container=timeseries_test) ## /!\\ very important step : make sure the right timeserie is used here\n",
    "\n",
    "X_test = pd.DataFrame(index=y_test.index) \n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f971a-3d46-465d-9c6d-c7d299eee493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# display results\n",
    "print(classification_report(y_test.values, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fca54-c906-46b3-a8ed-57738ccb52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4128b-a18c-4b39-8586-52065bd2c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities \n",
    "X_test = pd.DataFrame(index=y_test.index)\n",
    "pipeline.set_params(augmenter__timeseries_container=timeseries_test)\n",
    "proba = pipeline.predict_proba(X_test)\n",
    "proba_df = pd.DataFrame(data=proba) #store result in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c8fb7-d14b-41bd-8d64-2e5fbec2ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    " # rename columns with actual pipeline classes\n",
    "for cl in range(0,len(pipeline.classes_)):\n",
    "    proba_df=proba_df.rename(columns={cl: pipeline.classes_[cl]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d070bc4-2f4a-4a95-a39a-eecacd6a1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together in a nice dataframe to view result per field\n",
    "# y_test=gdf_test[['id','TillageType']]\n",
    "# y_test=y_test.drop_duplicates(subset=['id'],keep='first')\n",
    "df_pred = pd.DataFrame(y_pred,columns=['Prediction'])\n",
    "\n",
    "df_pred = pd.DataFrame(y_pred,columns=['Prediction'])\n",
    "df=df_pred.join(y_test.reset_index())\n",
    "df=df.join(proba_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83919816-b43f-485a-a500-4aa844044b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv\n",
    "df.to_csv('data/09_processed/results_test_newmodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af1cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
