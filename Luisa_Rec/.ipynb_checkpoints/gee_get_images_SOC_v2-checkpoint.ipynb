{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython.display as disp\n",
    "import geopandas as gpd\n",
    "from geemap import geopandas_to_ee\n",
    "import pandas as pd\n",
    "import logging\n",
    "import ee\n",
    "import numpy as np\n",
    "from pygeosys.util.dataframe import chunk_dataframe\n",
    "import folium\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:/Users/lwh/OneDrive - GEOSYS/Bureau/Documents/PROJECTS/tillage_detection/tillage-detection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # GEE logging & initialization\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "# service_account = 'aubin-allies-sandbox@earthengine-185413.iam.gserviceaccount.com'\n",
    "# credentials = ee.ServiceAccountCredentials(service_account, '/home/lwh/Documents/aubin-allies-sandbox-earthengine-185413-d486e9739448.json')\n",
    "ee.Initialize()\n",
    "logger = logging.getLogger()\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Define input parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "input_filepath = 'data/00_external/Demo_account/Fields_demo_Account_US_only.geojson'\n",
    "chunk_size = None #to change depending on the size of the shapefile : None means there will be one batch of the default size (which is a 100)\n",
    "\n",
    "\n",
    "# load input vector dataset\n",
    "geometry_collection = gpd.read_file(input_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Parameters for cloud masking\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 50\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 1\n",
    "BUFFER = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2 collection with cloud probability\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clouds band\n",
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clouds shadows band\n",
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions GEE display with folium\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define a method for displaying Earth Engine image tiles to a folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name, show=True, opacity=1, min_zoom=0):\n",
    "    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "    folium.raster_layers.TileLayer(\n",
    "        tiles=map_id_dict['tile_fetcher'].url_format,\n",
    "        attr='Map Data Â© <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "        name=name,\n",
    "        show=show,\n",
    "        opacity=opacity,\n",
    "        min_zoom=min_zoom,\n",
    "        overlay=True,\n",
    "        control=True\n",
    "        ).add_to(self)\n",
    "\n",
    "# Add the Earth Engine layer method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDVI(image):\n",
    "  return image.addBands(image.normalizedDifference(['B8', 'B4']).rename('NDVI'))\n",
    "def NBR2(image):\n",
    "    return image.addBands(image.normalizedDifference(['B11', 'B12']).rename('NBR2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunk_size is None:\n",
    "    geometry_collections = [geometry_collection]\n",
    "else:\n",
    "    geometry_collections = list(chunk_dataframe(geometry_collection, chunk_size))\n",
    "logger.info(f'Chunks: {len(geometry_collections)}')\n",
    "filename_prefixes = []\n",
    "\n",
    "output_folder='data/04_features/'\n",
    "year=2020\n",
    "\n",
    "# loop on chunks\n",
    "for chunk_id,  gc in enumerate(geometry_collections):\n",
    "    logger.info(\n",
    "        f'Processing chunk {chunk_id} / {len(geometry_collections)}')\n",
    "    logger.info(\n",
    "        f'Uploading FeatureCollection ({len(gc)} Features) on server side')\n",
    "    feature_collection_ee = geopandas_to_ee(gc)\n",
    "\n",
    "    logger.info('Creating Task')\n",
    "\n",
    "    feature = feature_collection_ee.first()\n",
    "    \n",
    "    s2_sr_cld_col = get_s2_sr_cld_col(feature.geometry(), f'{year}-01-01', f'{year}-12-31')\n",
    "    s2_sr_cld_col=s2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask).map(NDVI)\n",
    "\n",
    "    # function to retrive available dates\n",
    "    acq_times = s2_sr_cld_col.aggregate_array('GENERATION_TIME').getInfo()\n",
    "    list_date = [time.strftime('%x', time.gmtime(acq_time/1000)) for acq_time in acq_times]\n",
    "\n",
    "\n",
    "    for date in tqdm(list_date):\n",
    "        date_reformat = f'{year}-{date[0:2]}-{date[3:5]}'\n",
    "        date_reformat_2 = f'{year}-{date[0:2]}-{int(date[3:5])+1}'\n",
    "        \n",
    "        Im_final=ee.Image(s2_sr_cld_col.filterDate(f'{date_reformat}', f'{date_reformat_2}').filterBounds(feature_collection_ee).select('NDVI').median())\n",
    "        output_filename = f\"tillage/\" #where to put it in the bucket\n",
    "        fullName=f\"S2_image_NDVI_{date_reformat}\"\n",
    "        task=ee.batch.Export.image.toCloudStorage(**{\n",
    "            'image': Im_final,\n",
    "            'description': fullName,\n",
    "            'bucket': 'gri_geosys',\n",
    "            'fileNamePrefix':output_filename+fullName,\n",
    "            'scale': 10,\n",
    "            'region':feature.geometry(),\n",
    "            'maxPixels': 15000000000,\n",
    "            'skipEmptyTiles': True\n",
    "            })\n",
    "        task.start()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2020\n",
    "date_start = f'{year}-01-01' \n",
    "date_end = f'{year}-12-31'\n",
    "\n",
    "chunk_size=None\n",
    "if chunk_size is None:\n",
    "    geometry_collections = [geometry_collection]\n",
    "else:\n",
    "    geometry_collections = list(chunk_dataframe(geometry_collection, chunk_size))\n",
    "\n",
    "print(f'Chunks: {len(geometry_collections)}')\n",
    "filename_prefixes = []\n",
    "for chunk_id,  gc in enumerate(geometry_collections):\n",
    "    print(\n",
    "            f'Processing chunk {chunk_id+1} / {len(geometry_collections)}')\n",
    "    print(\n",
    "            f'Uploading FeatureCollection ({len(gc)} Features) on server side')\n",
    "    table = geopandas_to_ee(gc)\n",
    "\n",
    "\n",
    "aoi = table.geometry()\n",
    "aoi_sub=table.first().geometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "im_coll = dataset.filterBounds(\n",
    "    aoi_sub).filterDate(ee.Date(date_start),ee.Date(date_end)\n",
    "    ).filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')\n",
    "    ).sort('system:time_start')\n",
    "im_list = im_coll.toList(im_coll.size())\n",
    "acq_times = im_coll.aggregate_array('system:time_start').getInfo()\n",
    "dates = [time.strftime('%x', time.gmtime(acq_time/1000)) for acq_time in acq_times]\n",
    "cpt = 0\n",
    "for date in tqdm(dates) : \n",
    "    date_reformat = f'{year}-{date[0:2]}-{date[3:5]}'\n",
    "    im_list = im_coll.toList(im_coll.size())\n",
    "    im_final = ee.Image(im_list.get(cpt)).select('VV','VH').clip(aoi_sub)\n",
    "    cpt+=1\n",
    "    output_filename = f\"tillage/\"\n",
    "    fullName=f\"S1_image_{date_reformat}\"\n",
    "    task=ee.batch.Export.image.toCloudStorage(**{\n",
    "        'image': im_final,\n",
    "        'description': fullName,\n",
    "        'bucket': 'gri_geosys',\n",
    "        'fileNamePrefix':output_filename+fullName,\n",
    "        'scale': 10,\n",
    "        'region':feature.geometry(),\n",
    "        'maxPixels': 15000000000,\n",
    "        'skipEmptyTiles': True\n",
    "        })\n",
    "    task.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bare_soil_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8dd5d7dea1d432df401259b1c2a929100ec24d3248b88c55906a5b7354228eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
